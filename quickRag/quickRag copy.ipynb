{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/readme_sections \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "UnexpectedResponse",
     "evalue": "Unexpected Response: 404 (Not Found)\nRaw response content:\nb'{\"status\":{\"error\":\"Not found: Collection `readme_sections` doesn\\'t exist!\"},\"time\":0.000011626}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedResponse\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 245\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# FastAPI app\u001b[39;00m\n\u001b[0;32m    244\u001b[0m app \u001b[38;5;241m=\u001b[39m FastAPI()\n\u001b[1;32m--> 245\u001b[0m readme_processor \u001b[38;5;241m=\u001b[39m READMEProcessor()\n\u001b[0;32m    247\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/process_readme\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_readme\u001b[39m(file: UploadFile \u001b[38;5;241m=\u001b[39m File(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)):\n\u001b[0;32m    249\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m file\u001b[38;5;241m.\u001b[39mread()\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mREADMEProcessor.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdrant_client \u001b[38;5;241m=\u001b[39m QdrantClient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6333\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_collection()\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mREADMEProcessor._setup_collection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_collection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdrant_client\u001b[38;5;241m.\u001b[39mget_collection(COLLECTION_NAME):\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdrant_client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m     42\u001b[0m             collection_name\u001b[38;5;241m=\u001b[39mCOLLECTION_NAME,\n\u001b[0;32m     43\u001b[0m             vectors_config\u001b[38;5;241m=\u001b[39mVectorParams(size\u001b[38;5;241m=\u001b[39mVECTOR_SIZE, distance\u001b[38;5;241m=\u001b[39mDistance\u001b[38;5;241m.\u001b[39mCOSINE)\n\u001b[0;32m     44\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:1944\u001b[0m, in \u001b[0;36mQdrantClient.get_collection\u001b[1;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get detailed information about specified existing collection\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \n\u001b[0;32m   1936\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;124;03m    Detailed information about the collection\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_collection(collection_name\u001b[38;5;241m=\u001b[39mcollection_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\qdrant_remote.py:2436\u001b[0m, in \u001b[0;36mQdrantRemote.get_collection\u001b[1;34m(self, collection_name, **kwargs)\u001b[0m\n\u001b[0;32m   2429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefer_grpc:\n\u001b[0;32m   2430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GrpcToRest\u001b[38;5;241m.\u001b[39mconvert_collection_info(\n\u001b[0;32m   2431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrpc_collections\u001b[38;5;241m.\u001b[39mGet(\n\u001b[0;32m   2432\u001b[0m             grpc\u001b[38;5;241m.\u001b[39mGetCollectionInfoRequest(collection_name\u001b[38;5;241m=\u001b[39mcollection_name),\n\u001b[0;32m   2433\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m   2434\u001b[0m         )\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m   2435\u001b[0m     )\n\u001b[1;32m-> 2436\u001b[0m result: Optional[types\u001b[38;5;241m.\u001b[39mCollectionInfo] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mcollections_api\u001b[38;5;241m.\u001b[39mget_collection(\n\u001b[0;32m   2437\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name\n\u001b[0;32m   2438\u001b[0m )\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m   2439\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet collection returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:1314\u001b[0m, in \u001b[0;36mSyncCollectionsApi.get_collection\u001b[1;34m(self, collection_name)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_collection\u001b[39m(\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1309\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse2005:\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m    Get detailed information about specified existing collection\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_for_get_collection(\n\u001b[0;32m   1315\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m   1316\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:397\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_get_collection\u001b[1;34m(self, collection_name)\u001b[0m\n\u001b[0;32m    392\u001b[0m path_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(collection_name),\n\u001b[0;32m    394\u001b[0m }\n\u001b[0;32m    396\u001b[0m headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    398\u001b[0m     type_\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mInlineResponse2005,\n\u001b[0;32m    399\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    400\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/collections/\u001b[39m\u001b[38;5;132;01m{collection_name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    401\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders \u001b[38;5;28;01mif\u001b[39;00m headers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    402\u001b[0m     path_params\u001b[38;5;241m=\u001b[39mpath_params,\n\u001b[0;32m    403\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:79\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, type_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:102\u001b[0m, in \u001b[0;36mApiClient.send\u001b[1;34m(self, request, type_)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedResponse\u001b[38;5;241m.\u001b[39mfor_response(response)\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m: Unexpected Response: 404 (Not Found)\nRaw response content:\nb'{\"status\":{\"error\":\"Not found: Collection `readme_sections` doesn\\'t exist!\"},\"time\":0.000011626}'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 768  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    try:\n",
    "        vector = get_embedding(section.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for section '{section.heading}': {e}\")\n",
    "        return\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "    logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "\n",
    "knn_model: Optional[NearestNeighbors] = None\n",
    "point_id_mapping: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    logger.info(\"Building KNN index...\")\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    \n",
    "    if not all_points or not all_points[0]:\n",
    "        logger.warning(\"No points found in the collection. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"Embeddings array is empty. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "    logger.info(f\"KNN index built successfully with {len(point_id_mapping)} points.\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata.get('tfidf_similarity', 0.0),\n",
    "            section.metadata.get('semantic_similarity', 0.0),\n",
    "            section.metadata.get('centrality', 0.0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model is None:\n",
    "        logger.warning(\"KNN model is not built. No search can be performed.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "    logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    build_knn_index()  # This will now handle empty collections gracefully\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\dima\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\dima\\anaconda3\\lib\\site-packages (from xgboost) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' already exists.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\dima\\anaconda3\\lib\\site-packages (0.115.0)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\dima\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dima\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in c:\\users\\dima\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dima\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\dima\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\dima\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\dima\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from fastapi) (0.38.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from fastapi) (4.9.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx[http2]>=0.20.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.45.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dima\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dima\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dima\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dima\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dima\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\dima\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dima\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' exists.\n",
      "INFO:__main__:Training XGBRanker is not implemented. Using default model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-21 (run_server):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dima\\anaconda3\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Dima\\anaconda3\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Dima\\AppData\\Local\\Temp\\ipykernel_3212\\121075372.py\", line 696, in run_server\n",
      "  File \"C:\\Users\\Dima\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 45, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dima\\anaconda3\\Lib\\asyncio\\events.py\", line 677, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-21 (run_server)'.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 768  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; adjust based on SentenceTransformer model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000, offset=scroll_response.next_page_offset)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Dependencies\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 768  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; will be updated based on model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(\n",
    "                    collection_name=COLLECTION_NAME_README,\n",
    "                    limit=10000,\n",
    "                    offset=scroll_response.next_page_offset\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will hit groq and retrieve a JSON object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:\n",
    "\n",
    "\n",
    "- this is a test MD and this code will generate metadata and a title that you can then feed into the Gravrag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted JSON part: {\n",
      "    \"title\": \"Comprehensive Description of Groqy Backend\",\n",
      "    \"summary\": \"This document provides a detailed overview of the Groqy backend, including its components, setup, utility functions, routes, frontend components, and security considerations.\",\n",
      "    \"keywords\": [\"Groqy\", \"backend\", \"components\", \"setup\", \"utility functions\", \"routes\", \"frontend components\", \"security considerations\"],\n",
      "    \"created_at\": \"2024-10-07T12:34:56Z\"\n",
      "}\n",
      "Comprehensive Description of Groqy Backend\n",
      "{'objective_id': 'project_x', 'user_id': 'user_123'}\n"
     ]
    }
   ],
   "source": [
    "#!pip install groq\n",
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "#following metadata is required:\n",
    "#abstitle\n",
    "#summary\n",
    "#keywords\n",
    "#created_at\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_metadata(response):\n",
    "    # Try to isolate the JSON part of the response\n",
    "    try:\n",
    "        # Use regex to find the JSON block between triple backticks\n",
    "        json_match = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "\n",
    "        if json_match:\n",
    "            json_part = json_match.group(1).strip()  # Extract the JSON string\n",
    "        else:\n",
    "            raise ValueError(\"Could not find the JSON block in the response.\")\n",
    "\n",
    "        # Print the extracted JSON part for debugging\n",
    "        print(\"Extracted JSON part:\", json_part)\n",
    "\n",
    "        # Parse the JSON string to a Python dictionary\n",
    "        metadata = json.loads(json_part)\n",
    "\n",
    "        # Extract the title\n",
    "        title = metadata[\"title\"]\n",
    "\n",
    "        # Return title as a string and metadata as a JSON object (dictionary)\n",
    "        return title, metadata\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        return \"Error: Invalid JSON response.\", None\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        return \"Error: Could not extract metadata.\", None\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "        return \"Error: Could not extract metadata.\", None\n",
    "    \n",
    "\n",
    "# Initialize client and pass the API key directly\n",
    "client = Groq(api_key=\"wedontcommitapikeys;)\")\n",
    "\n",
    "System_prompt = \"\"\"You are Groq, an advanced AI capable of analyzing Markdown (MD) text inputs and generating creative outputs. When you receive an MD file, your task is to extract key themes, ideas, and insights to create a unique title that encapsulates the essence of the content.\n",
    "\n",
    "Based on the analysis of the MD text, generate a metadata JSON object containing:\n",
    "\n",
    "- **title**: A concise, engaging title that reflects the main topic or idea of the MD content.\n",
    "- **summary**: A brief summary (1-2 sentences) highlighting the core message or purpose of the document.\n",
    "- **keywords**: An array of relevant keywords or phrases derived from the MD content that represent its key concepts.\n",
    "- **created_at**: A timestamp indicating when the metadata was generated.\n",
    "\n",
    "Make your title catchy and informative, and ensure that the metadata accurately represents the content of the MD file.\n",
    "\n",
    "Heres an example of the expected output format:\n",
    "{\n",
    "    \"title\": \"Engaging Title Here\",\n",
    "    \"metadata = {\n",
    "    \"summary\": \"This document discusses key insights into ...\",\n",
    "    \"keywords\": \"keyword1\", \"keyword2\", \"keyword3\",\n",
    "    \"created_at\": \"2024-10-07T12:34:56Z\"}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "User_prompt = \"\"\"\n",
    "1. [Overview](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "2. [Backend Components](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Schemas](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [User Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Project Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Task Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Submission Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Comment Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Notification Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Email Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Leaderboard Schema](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Server Setup](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Imports and Dependencies](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Express Application Configuration](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Middleware Configuration](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [File Upload Handling](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Authentication Middleware](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Request Logging](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Utility Functions](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [createNewTask Function](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Authentication Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [User Registration](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [User Login](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Promote User to Admin](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get Current User Profile](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Task Management Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Create New Task](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Update Task](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get All Tasks for User](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Delete Task](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Submission Management Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get Submissions by User](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Project Management Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Create New Project](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get All Projects](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get Specific Project by ID](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Update Project](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Delete Project](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Comment Management Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Add a Comment to a Task](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get Comments for a Task](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Admin Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Get All Users](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Update a User](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Delete a User](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Admin Overview](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Admin Task Status](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Admin User Activity](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Admin Analytics Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "                - [User Activity Analytics](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "                - [Task Completion Analytics](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Admin Task Assignment](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Miscellaneous Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [File Upload Endpoint](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Processor Endpoint](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Frontend Routes](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "3. [Frontend Components](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [HTML Structure](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [CSS and Styling](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [JavaScript Functionality](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Background Effects](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Navigation Bar](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Main Content](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Initial View](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Pitch Form](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Pitch Deck Slides](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "        - [Interactive Features](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Star Generation](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Pitch Form Handling](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Pitch Deck Generation](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Slide Navigation](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Tooltips](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Invest Modal](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "            - [Keyboard and Swipe Navigation](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "4. [Security Considerations](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Authentication and Authorization](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [Data Sanitization](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "    - [File Handling Security](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "5. [Deployment and Environment Configuration](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "6. [Conclusion](https://www.notion.so/Comprehensive-description-of-groqy-backend-116546104b0e8014a407e02c3450db1a?pvs=21)\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{System_prompt}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{User_prompt}\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "result = (chat_completion.choices[0].message.content)\n",
    "\n",
    "titleT, metadataT = extract_metadata(result)\n",
    "print(title)\n",
    "print(metadata)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will create memory inside the RAG using Query:\n",
    "\n",
    "- recall using query when you dont know the specific metadata - e.g. like in SQL if you didnt know for some reason your table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Memory created successfully'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_request(content=User_prompt):\n",
    "    url = f\"{baseUrl}/gravrag/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": content,\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "    \n",
    "send_request(\"Another test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will create memory inside the RAG using metadata:\n",
    "- recall using query when you know the specific metadata - e.g. like in SQL if you DO know the table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Memory created successfully'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/gravrag/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "    \n",
    "\n",
    "#send_request_with_metadata(title,metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will Recall memory using Just the Query (Recall Memory Based on Semantic Search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memories': [{'content': 'Some query', 'metadata': {'objective_id': 'project_x', 'user_id': 'user_123', 'timestamp': 1728271252.7177944, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 1.0, 'gravitational_pull': 0.9999999906888026, 'spacetime_coordinate': 0.07861617004577379}}, {'content': 'Some query', 'metadata': {'objective_id': 'project_x', 'user_id': 'user_123', 'timestamp': 1728267041.1143515, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 1.0, 'gravitational_pull': 0.9999999906888026, 'spacetime_coordinate': 0.0002367242378170876}}, {'content': 'Some query', 'metadata': {'objective_id': 'project_x', 'user_id': 'user_123', 'timestamp': 1728269247.933541, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 1.0, 'gravitational_pull': 0.9999999906888026, 'spacetime_coordinate': 0.0004956616470272158}}, {'content': 'Another test', 'metadata': {'timestamp': 1728271231.9067838, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 0.34780788058748113, 'gravitational_pull': 0.34780788974707594, 'spacetime_coordinate': 0.010372096117494485}}, {'content': 'Another test', 'metadata': {'timestamp': 1728269607.8867402, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 0.34780788058748113, 'gravitational_pull': 0.34780788974707594, 'spacetime_coordinate': 0.00020983212765635412}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def recall_memory(query, topK):\n",
    "    url = f\"{baseUrl}/gravrag/recall_memory\"\n",
    "    \n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": topK\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "response = recall_memory(\"Some query\", topK=5)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will Recall memory using Just the Metadata (Recall Memory with Metadata Matching):\n",
    "\n",
    "- for this to work you need to match both the QUERY and the METADATA EXACTLY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memories': [{'content': 'Some query', 'metadata': {'objective_id': 'project_x', 'user_id': 'user_123', 'timestamp': 1728267041.1143515, 'recall_count': 0, 'memetic_similarity': 1.0, 'semantic_relativity': 1.0, 'gravitational_pull': 0.9999999906888026, 'spacetime_coordinate': 0.00023622207143356419}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def recall_memory_with_metadata(query, metadata, top_k):\n",
    "    url = f\"{baseUrl}/gravrag/recall_with_metadata\"\n",
    "    \n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Define the body, including top_k\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"metadata\": metadata,\n",
    "        \"top_k\": top_k  # Include top_k in the body\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "metadata = {\n",
    "    \"objective_id\": \"project_x\",\n",
    "    \"user_id\": \"user_123\"\n",
    "}\n",
    "top_k = 1\n",
    "\n",
    "response = recall_memory_with_metadata(\"Some query\", metadata, top_k)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
