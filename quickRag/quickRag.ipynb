{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 384  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 384\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    try:\n",
    "        vector = get_embedding(section.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for section '{section.heading}': {e}\")\n",
    "        return\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "    logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "\n",
    "knn_model: Optional[NearestNeighbors] = None\n",
    "point_id_mapping: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    logger.info(\"Building KNN index...\")\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    \n",
    "    if not all_points or not all_points[0]:\n",
    "        logger.warning(\"No points found in the collection. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"Embeddings array is empty. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "    logger.info(f\"KNN index built successfully with {len(point_id_mapping)} points.\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata.get('tfidf_similarity', 0.0),\n",
    "            section.metadata.get('semantic_similarity', 0.0),\n",
    "            section.metadata.get('centrality', 0.0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model is None:\n",
    "        logger.warning(\"KNN model is not built. No search can be performed.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "    logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     build_knn_index()  # This will now handle empty collections gracefully\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Using cached xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: xgboost in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (1.6.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: sentence-transformers in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (2.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx>=0.20.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.26.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (69.5.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.3)\n",
      "Requirement already satisfied: h2<5,>=3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Rolling back uninstall of protobuf\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\json_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\json_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message_factory.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message_factory.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\any_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\any_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\api_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\api_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\compiler\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~ompiler\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pool.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pool.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\duration_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\duration_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\empty_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\empty_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\field_mask_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\field_mask_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\_parameterized.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\_parameterized.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\api_implementation.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\api_implementation.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\containers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\decoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\encoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\enum_type_wrapper.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\enum_type_wrapper.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\extension_dict.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\extension_dict.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\field_mask.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\field_mask.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\message_listener.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\message_listener.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\numpy\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\internal\\~umpy\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\python_message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\type_checkers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\well_known_types.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\wire_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\wire_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\json_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\json_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message_factory.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message_factory.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\proto_builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\proto_builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\pyext\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~yext\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service_reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service_reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\source_context_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\source_context_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\struct_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\struct_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\symbol_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_encoding.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_encoding.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\timestamp_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\timestamp_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\type_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\type_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\unknown_fields.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\unknown_fields.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\util\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~til\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\wrappers_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\wrappers_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-u_83ebsj\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3.dist-info\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~rotobuf-4.25.3.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Users\\\\nasan\\\\anaconda3\\\\envs\\\\myenv\\\\Lib\\\\site-packages\\\\google\\\\_upb\\\\_message.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' already exists.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' exists.\n",
      "INFO:__main__:Training XGBRanker is not implemented. Using default model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [35356]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; adjust based on SentenceTransformer model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000, offset=scroll_response.next_page_offset)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; will be updated based on model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(\n",
    "                    collection_name=COLLECTION_NAME_README,\n",
    "                    limit=10000,\n",
    "                    offset=scroll_response.next_page_offset\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [44932]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "# !pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Initialize SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "VECTOR_SIZE = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Create Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "# Function to send request with metadata\n",
    "import json\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            logger.error(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match=MatchValue(value=section_heading))]\n",
    "        )\n",
    "        results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results:\n",
    "            return {}\n",
    "\n",
    "        section = results[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=section['parent']))]\n",
    "            )\n",
    "            parent_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results:\n",
    "                context[\"parent\"] = parent_results[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/create_memory\")\n",
    "async def create_memory_api(content: str, metadata: Dict[str, Any]):\n",
    "    try:\n",
    "        vector = embedding_model.encode(content).tolist()\n",
    "        point_id = str(uuid.uuid4())\n",
    "        payload = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        return {\"message\": \"Memory created successfully\", \"id\": point_id}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating memory: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create memory.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Function to process README and send sections to database\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    sections = parse_readme(content)\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        # Generate embedding\n",
    "        section.vector = embedding_model.encode(section.content).tolist() if section.content else None\n",
    "\n",
    "        # Prepare title and metadata\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Send to database\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to database. Response: {response}\")\n",
    "\n",
    "        # Optionally, add to Qdrant directly\n",
    "        add_section_to_qdrant(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88176f6912f749389ce6d70c340f6cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID 9b629c23-2901-4794-b8ca-d58bebb5fb3f.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3acbd350dc4ba6b44ded3ff7fdd2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 2efdec15-3fbd-4e8f-858a-aa7cbb39c639.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec115f6f98934720889a948b9c8e0e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID 1dcf1ef9-21df-4927-8e55-621a508722d2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd346be56f348b69d46857f56d0c359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID cb764f5c-c5e6-4d83-89d0-a17dc28991e1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d9d6ff1fee49cd9f3efb915f00abdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID befe5248-b91c-4df2-a05d-1cc71eabef78.\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/advanced_readme_sections/points/scroll \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:Error getting context for section 'Installation': 'list' object has no attribute 'payload'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n",
      "\n",
      "Retrieved Context:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "# Sample README content (you can replace this with the path to your actual README file)\n",
    "sample_readme_content = \"\"\"\n",
    "# Sample Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a sample README file for testing purposes.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Instructions to install...\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "List of prerequisites...\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "Step-by-step installation guide...\n",
    "\n",
    "## Usage\n",
    "\n",
    "How to use the project...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save the sample README content to a file\n",
    "readme_path = \"sample_README.md\"\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_readme_content)\n",
    "\n",
    "# Wait for the server to start\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "process_readme_and_send(readme_path)\n",
    "\n",
    "# Retrieve context for a section\n",
    "section_heading = \"Installation\"\n",
    "context = get_context(section_heading, depth=1)\n",
    "\n",
    "# Print the context\n",
    "import json\n",
    "print(\"\\nRetrieved Context:\")\n",
    "print(json.dumps(context, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<Server.serve() done, defined at d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py:63> exception=SystemExit(1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 160, in startup\n",
      "    server = await loop.create_server(\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1519, in create_server\n",
      "    raise OSError(err.errno, 'error while attempting '\n",
      "OSError: [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\nasan\\AppData\\Local\\Temp\\ipykernel_44932\\2900098384.py\", line 262, in run_server\n",
      "    loop.run_until_complete(server.serve())\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\tasks.py\", line 315, in __wakeup\n",
      "    self.__step()\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 78, in serve\n",
      "    await self.startup(sockets=sockets)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 170, in startup\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Creating collection 'advanced_readme_sections' with vector size 384.\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 409 Conflict\"\n"
     ]
    },
    {
     "ename": "UnexpectedResponse",
     "evalue": "Unexpected Response: 409 (Conflict)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Collection `advanced_readme_sections` already exists!\"},\"time\":0.00003676}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m, in \u001b[0;36minitialize_qdrant_collection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m collection \u001b[38;5;241m=\u001b[39m qdrant_client\u001b[38;5;241m.\u001b[39mget_collection(COLLECTION_NAME)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m VECTOR_SIZE:\n\u001b[0;32m     59\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExisting collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has different vector size. Recreating collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\pydantic\\main.py:811\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CollectionInfo' object has no attribute 'vectors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Initialize the Qdrant collection\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43minitialize_qdrant_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Define Data Models\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReadmeSection\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m, in \u001b[0;36minitialize_qdrant_collection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with vector size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVECTOR_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mqdrant_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVectorParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVECTOR_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOSINE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\qdrant_client.py:2096\u001b[0m, in \u001b[0;36mQdrantClient.create_collection\u001b[1;34m(self, collection_name, vectors_config, sparse_vectors_config, shard_number, sharding_method, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, timeout, **kwargs)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create empty collection with given parameters\u001b[39;00m\n\u001b[0;32m   2048\u001b[0m \n\u001b[0;32m   2049\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;124;03m    Operation result\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m   2097\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m   2098\u001b[0m     vectors_config\u001b[38;5;241m=\u001b[39mvectors_config,\n\u001b[0;32m   2099\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39mshard_number,\n\u001b[0;32m   2100\u001b[0m     sharding_method\u001b[38;5;241m=\u001b[39msharding_method,\n\u001b[0;32m   2101\u001b[0m     replication_factor\u001b[38;5;241m=\u001b[39mreplication_factor,\n\u001b[0;32m   2102\u001b[0m     write_consistency_factor\u001b[38;5;241m=\u001b[39mwrite_consistency_factor,\n\u001b[0;32m   2103\u001b[0m     on_disk_payload\u001b[38;5;241m=\u001b[39mon_disk_payload,\n\u001b[0;32m   2104\u001b[0m     hnsw_config\u001b[38;5;241m=\u001b[39mhnsw_config,\n\u001b[0;32m   2105\u001b[0m     optimizers_config\u001b[38;5;241m=\u001b[39moptimizers_config,\n\u001b[0;32m   2106\u001b[0m     wal_config\u001b[38;5;241m=\u001b[39mwal_config,\n\u001b[0;32m   2107\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[0;32m   2108\u001b[0m     init_from\u001b[38;5;241m=\u001b[39minit_from,\n\u001b[0;32m   2109\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   2110\u001b[0m     sparse_vectors_config\u001b[38;5;241m=\u001b[39msparse_vectors_config,\n\u001b[0;32m   2111\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2112\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\qdrant_remote.py:2647\u001b[0m, in \u001b[0;36mQdrantRemote.create_collection\u001b[1;34m(self, collection_name, vectors_config, shard_number, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, timeout, sparse_vectors_config, sharding_method, **kwargs)\u001b[0m\n\u001b[0;32m   2630\u001b[0m     init_from \u001b[38;5;241m=\u001b[39m GrpcToRest\u001b[38;5;241m.\u001b[39mconvert_init_from(init_from)\n\u001b[0;32m   2632\u001b[0m create_collection_request \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mCreateCollection(\n\u001b[0;32m   2633\u001b[0m     vectors\u001b[38;5;241m=\u001b[39mvectors_config,\n\u001b[0;32m   2634\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39mshard_number,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2644\u001b[0m     sharding_method\u001b[38;5;241m=\u001b[39msharding_method,\n\u001b[0;32m   2645\u001b[0m )\n\u001b[1;32m-> 2647\u001b[0m result: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_collection_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate collection returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:1170\u001b[0m, in \u001b[0;36mSyncCollectionsApi.create_collection\u001b[1;34m(self, collection_name, timeout, create_collection)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1163\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1164\u001b[0m     timeout: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1165\u001b[0m     create_collection: m\u001b[38;5;241m.\u001b[39mCreateCollection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse200:\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;124;03m    Create new collection with given parameters\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_for_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:116\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_create_collection\u001b[1;34m(self, collection_name, timeout, create_collection)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[0;32m    115\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInlineResponse200\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py:79\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py:102\u001b[0m, in \u001b[0;36mApiClient.send\u001b[1;34m(self, request, type_)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedResponse\u001b[38;5;241m.\u001b[39mfor_response(response)\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m: Unexpected Response: 409 (Conflict)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Collection `advanced_readme_sections` already exists!\"},\"time\":0.00003676}'"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "# You can run this in your terminal or uncomment the following line if using Jupyter.\n",
    "# !pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Define the collection name\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "\n",
    "# Initialize SentenceTransformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "VECTOR_SIZE = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Create Collection if it doesn't exist, else verify vector size\n",
    "def initialize_qdrant_collection():\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "        if collection.vectors.size != VECTOR_SIZE:\n",
    "            logger.info(f\"Existing collection '{COLLECTION_NAME}' has different vector size. Recreating collection.\")\n",
    "            qdrant_client.recreate_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "            logger.info(f\"Collection '{COLLECTION_NAME}' recreated with vector size {VECTOR_SIZE}.\")\n",
    "        else:\n",
    "            logger.info(f\"Collection '{COLLECTION_NAME}' already exists with correct vector size.\")\n",
    "    except Exception:\n",
    "        logger.info(f\"Creating collection '{COLLECTION_NAME}' with vector size {VECTOR_SIZE}.\")\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "        )\n",
    "        logger.info(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "# Initialize the Qdrant collection\n",
    "initialize_qdrant_collection()\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class CreateMemoryRequest(BaseModel):\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class CreateMemoryResponse(BaseModel):\n",
    "    message: str\n",
    "    id: Optional[str] = None\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    score: float\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    results: List[SearchResult]\n",
    "\n",
    "class ContextResponse(BaseModel):\n",
    "    context: Dict[str, Any]\n",
    "\n",
    "class PruneRequest(BaseModel):\n",
    "    threshold: float = 0.5\n",
    "    max_age_days: int = 30\n",
    "\n",
    "class PruneResponse(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class RebuildKNNResponse(BaseModel):\n",
    "    message: str\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "\n",
    "@app.post(\"/create_memory\", response_model=CreateMemoryResponse)\n",
    "async def create_memory_api(request: CreateMemoryRequest):\n",
    "    try:\n",
    "        vector = embedding_model.encode(request.content).tolist()\n",
    "        point_id = str(uuid.uuid4())\n",
    "        payload = {\n",
    "            \"content\": request.content,\n",
    "            \"metadata\": request.metadata\n",
    "        }\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        return CreateMemoryResponse(message=\"Memory created successfully\", id=point_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating memory: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create memory.\")\n",
    "\n",
    "@app.post(\"/search\", response_model=SearchResponse)\n",
    "async def search_api(request: SearchRequest):\n",
    "    try:\n",
    "        query_vector = embedding_model.encode(request.query).tolist()\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=request.top_k\n",
    "        )\n",
    "        results = []\n",
    "        for hit in search_results:\n",
    "            # Calculate cosine similarity as semantic similarity\n",
    "            semantic_sim = cosine_similarity([query_vector], [hit.vector])[0][0]\n",
    "            # For demonstration, relevance_score is set as semantic_sim\n",
    "            relevance_score = semantic_sim\n",
    "            results.append(SearchResult(\n",
    "                content=hit.payload[\"content\"],\n",
    "                metadata=hit.payload[\"metadata\"],\n",
    "                score=relevance_score\n",
    "            ))\n",
    "        # Sort results by score descending\n",
    "        results_sorted = sorted(results, key=lambda x: x.score, reverse=True)\n",
    "        return SearchResponse(results=results_sorted)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\", response_model=ContextResponse)\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match=MatchValue(value=section_heading))]\n",
    "        )\n",
    "        results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results:\n",
    "            raise HTTPException(status_code=404, detail=\"Section not found.\")\n",
    "        \n",
    "        section = results[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "        \n",
    "        # Fetch parent section\n",
    "        parent_heading = section.get(\"parent\")\n",
    "        if parent_heading:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=parent_heading))]\n",
    "            )\n",
    "            parent_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results:\n",
    "                context[\"parent\"] = parent_results[0].payload\n",
    "        \n",
    "        # Fetch children sections\n",
    "        children_headings = section.get(\"children\", [])\n",
    "        for child_heading in children_headings:\n",
    "            child_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=child_heading))]\n",
    "            )\n",
    "            child_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=child_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if child_results:\n",
    "                context[\"children\"].append(child_results[0].payload)\n",
    "        \n",
    "        # Fetch siblings\n",
    "        if parent_heading:\n",
    "            sibling_headings = context[\"parent\"].get(\"children\", [])\n",
    "            for sibling_heading in sibling_headings:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_filter = Filter(\n",
    "                        must=[FieldCondition(key=\"heading\", match=MatchValue(value=sibling_heading))]\n",
    "                    )\n",
    "                    sibling_results = qdrant_client.scroll(\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                        scroll_filter=sibling_filter,\n",
    "                        limit=1\n",
    "                    )\n",
    "                    if sibling_results:\n",
    "                        context[\"siblings\"].append(sibling_results[0].payload)\n",
    "        \n",
    "        return ContextResponse(context=context)\n",
    "    except HTTPException as he:\n",
    "        raise he\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context for section '{section_heading}': {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\", response_model=PruneResponse)\n",
    "async def prune_api(prune_request: PruneRequest):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = prune_request.max_age_days * 24 * 60 * 60\n",
    "\n",
    "        # Define filter for low relevance_score and older than max_age_days\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=prune_request.threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "        return PruneResponse(message=\"Pruning completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\", response_model=RebuildKNNResponse)\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        # For Qdrant, the index is handled internally, so no action needed\n",
    "        # If you have a separate KNN model, rebuild it here\n",
    "        logger.info(\"KNN index rebuilt successfully (handled internally by Qdrant).\")\n",
    "        return RebuildKNNResponse(message=\"KNN index rebuilt successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Function to start the server\n",
    "def start_server():\n",
    "    server_thread = Thread(target=run_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    logger.info(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Initialize and start the server\n",
    "start_server()\n",
    "\n",
    "# Function to process README and send sections to database\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    try:\n",
    "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"README file not found at path: {readme_path}\")\n",
    "        return\n",
    "    \n",
    "    sections = parse_readme(content)\n",
    "    section_graph = build_section_graph(sections)\n",
    "    \n",
    "    for section in sections:\n",
    "        if section.content.strip():\n",
    "            # Generate embedding\n",
    "            section.vector = embedding_model.encode(section.content).tolist()\n",
    "        else:\n",
    "            section.vector = None  # Handle sections with no content\n",
    "    \n",
    "        # Prepare title and metadata\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": int(time.time()),\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0,\n",
    "                \"cluster\": section.metadata.get('cluster', 0),\n",
    "                \"centrality\": 0.0  # Placeholder, can be calculated if needed\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        if not section.vector:\n",
    "            logger.warning(f\"Section '{title}' has no content and will not be sent to the database.\")\n",
    "            continue\n",
    "    \n",
    "        # Send to database via API\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to database. Response: {response}\")\n",
    "    \n",
    "    logger.info(\"Finished processing and sending all sections.\")\n",
    "\n",
    "# Initialize a sample KNN model (if needed)\n",
    "# Currently, Qdrant handles KNN internally\n",
    "\n",
    "# Example: Function to retrieve context (already defined in API)\n",
    "\n",
    "# Note: The server is running in the background as a daemon thread and will terminate when the main program exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   python setup.py egg_info did not run successfully.\n",
      "   exit code: 1\n",
      "  > [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      " Encountered error while generating package metadata.\n",
      "> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn qdrant-client markdown beautifulsoup4 sklearn xgboost networkx nest_asyncio sentence-transformers numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a9582942e4528b7a6689b8df6ca4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'message': 'Memory created successfully', 'id': '0ba22cf0-adc7-4ffe-a7dd-865eebaac104'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935f7ab94d2a4ca0b79193d1f0478f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eee2fdb9274d32bcf77d96c34e56d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID cac72711-9ccb-4870-be47-c27d3690d443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'message': 'Memory created successfully', 'id': '75b5749a-ec96-441f-989e-5b4ca7ac245b'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e11e11570345e99e0cf35b2b387a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5138561735541f2b2726b9648644a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 57500e53-e235-4c13-9a10-e6fca60352cc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'message': 'Memory created successfully', 'id': 'c9286f86-cc79-4003-9ee4-701e1ee3ed48'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88690d581004687b10575540f2f96f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8811abac3927470d8ee42d6b4ec16b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID 6ad595c1-1605-4cec-bc7f-aa192857935e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'message': 'Memory created successfully', 'id': '825ba3f0-ccef-43cb-bc87-11fa4007e76f'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5291575c610405da44a289cc65f48c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a29cad9a24ff6bf833aa5fd3b4b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID 19c69a55-1e84-46de-8399-0ae2bd645657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'message': 'Memory created successfully', 'id': '767b950e-a53c-4bb5-8f98-22ab06213932'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b843d49898249558af1be5e10d5114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e05c40f1a54cbea1bee0d578706f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID a7614a63-35ea-4dbd-8149-2e0d8e71347e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'message': 'Memory created successfully', 'id': 'dcbda5b9-28db-4968-9350-1dcf062d566c'}\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Failed to retrieve context.\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5966d8bdd3b64fd4bb357ede943f7017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Search failed.\\\"}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the sample README\n",
    "readme_path = \"README.md\"\n",
    "\n",
    "# Sample README content\n",
    "sample_readme_content = \"\"\"\n",
    "# Sample Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a sample README file for testing purposes.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Instructions to install...\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "List of prerequisites...\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "Step-by-step installation guide...\n",
    "\n",
    "## Usage\n",
    "\n",
    "How to use the project...\n",
    "\"\"\"\n",
    "\n",
    "# Write the sample README content to the file\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_readme_content)\n",
    "\n",
    "# Wait briefly to ensure the server is up\n",
    "time.sleep(3)  # Adjust if necessary based on server startup time\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def run_processing():\n",
    "    process_readme_and_send(readme_path)\n",
    "\n",
    "processing_thread = Thread(target=run_processing)\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for the 'Installation' section\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search for \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Function to prune sections (optional)\n",
    "def prune_sections(threshold=0.5, max_age_days=30):\n",
    "    url = \"http://localhost:8000/prune\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"threshold\": threshold,\n",
    "        \"max_age_days\": max_age_days\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, prune sections\n",
    "# prune_response = prune_sections()\n",
    "# print(f\"\\nPrune Response: {prune_response}\")\n",
    "\n",
    "# Function to rebuild KNN index (optional)\n",
    "def rebuild_knn():\n",
    "    url = \"http://localhost:8000/rebuild_knn_index\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, rebuild KNN index\n",
    "# rebuild_response = rebuild_knn()\n",
    "# print(f\"\\nRebuild KNN Index Response: {rebuild_response}\")\n",
    "\n",
    "# Clean up: Remove the sample README file\n",
    "# os.remove(readme_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing README: add_section_to_qdrant() takes 1 positional argument but 2 were given\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Failed to retrieve context.\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4e9e0e9c2e4e69af5c86b5b81dfa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Search failed.\\\"}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the actual README (replace with the correct path)\n",
    "readme_path = \"README.md\"  # Make sure this points to your actual README\n",
    "\n",
    "# Wait briefly to ensure the server is up (optional adjustment if necessary)\n",
    "time.sleep(3)\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def process_readme_and_send(readme_path):\n",
    "    try:\n",
    "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "            readme_content = f.read()\n",
    "\n",
    "        # Use the actual processing function here to break down the README content\n",
    "        sections = parse_readme(readme_content)  # Assuming parse_readme function processes your README correctly\n",
    "\n",
    "        # Build the section graph (for context of parent-child relationships)\n",
    "        section_graph = build_section_graph(sections)\n",
    "\n",
    "        # Generate embeddings and metadata for each section\n",
    "        for section in sections:\n",
    "            if section.content:\n",
    "                section.vector = get_embedding(section.content).tolist()  # Get vector for the section\n",
    "                add_section_to_qdrant(section, section_graph)  # Add to database\n",
    "                print(f\"Processed and added section '{section.heading}' to the database.\")\n",
    "            else:\n",
    "                print(f\"Skipping section '{section.heading}' due to missing content.\")\n",
    "        \n",
    "        print(\"README processing completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing README: {str(e)}\")\n",
    "\n",
    "# Run the processing in a thread (optional, if necessary to avoid blocking)\n",
    "processing_thread = Thread(target=process_readme_and_send, args=(readme_path,))\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for a specific section, e.g., 'Installation'\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search query, for example \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Note: We have disabled any pruning and unnecessary operations to ensure the README is not altered in any way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6af923a7687428b8fa3b91910852da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID 9cff1cd4-a453-4578-a769-ba8eec5647b7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fa6332c1a247c0bfc647363fa161bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 680c30b8-5ad4-461b-bfd8-06b5ffa94a19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5ba539c70d4f2f995ac3563058a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID b2e8a395-e93d-40b2-89d1-e3d57625948e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cdb3bf5fe24444893237a3cb218842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID 4d201918-25a7-4b71-9106-c4b0e07c70a6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c88c360b84fcbb20bb90824692eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID fb1a01b9-146f-4a2c-a754-6b95fa69b2d7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 404\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Not Found\\\"}\"\n",
      "}\n",
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 404\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Not Found\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the sample README\n",
    "readme_path = \"sample_README.md\"\n",
    "\n",
    "# Sample README content\n",
    "sample_readme_content = \"\"\"\n",
    "# Sample Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a sample README file for testing purposes.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Instructions to install...\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "List of prerequisites...\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "Step-by-step installation guide...\n",
    "\n",
    "## Usage\n",
    "\n",
    "How to use the project...\n",
    "\"\"\"\n",
    "\n",
    "# Write the sample README content to the file\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_readme_content)\n",
    "\n",
    "# Wait briefly to ensure the server is up\n",
    "time.sleep(3)  # Adjust if necessary based on server startup time\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def run_processing():\n",
    "    process_readme_and_send(readme_path)\n",
    "\n",
    "processing_thread = Thread(target=run_processing)\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for the 'Installation' section\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search for \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Function to prune sections (optional)\n",
    "def prune_sections(threshold=0.5, max_age_days=30):\n",
    "    url = \"http://localhost:8000/prune\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"threshold\": threshold,\n",
    "        \"max_age_days\": max_age_days\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, prune sections\n",
    "# prune_response = prune_sections()\n",
    "# print(f\"\\nPrune Response: {prune_response}\")\n",
    "\n",
    "# Function to rebuild KNN index (optional)\n",
    "def rebuild_knn():\n",
    "    url = \"http://localhost:8000/rebuild_knn_index\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, rebuild KNN index\n",
    "# rebuild_response = rebuild_knn()\n",
    "# print(f\"\\nRebuild KNN Index Response: {rebuild_response}\")\n",
    "\n",
    "# Clean up: Remove the sample README file\n",
    "os.remove(readme_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
