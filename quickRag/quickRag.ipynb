{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 768  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:__main__:Building KNN index...\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/advanced_readme_sections/points/scroll \"HTTP/1.1 200 OK\"\n",
      "WARNING:__main__:No points found in the collection. KNN index not built.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 361\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[0;32m    360\u001b[0m build_knn_index()  \u001b[38;5;66;03m# This will now handle empty collections gracefully\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\main.py:587\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m     Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n\u001b[0;32m    589\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(config\u001b[38;5;241m.\u001b[39muds)  \u001b[38;5;66;03m# pragma: py-win32\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py:61\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: Optional[List[socket\u001b[38;5;241m.\u001b[39msocket]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    try:\n",
    "        vector = get_embedding(section.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for section '{section.heading}': {e}\")\n",
    "        return\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "    logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "\n",
    "knn_model: Optional[NearestNeighbors] = None\n",
    "point_id_mapping: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    logger.info(\"Building KNN index...\")\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    \n",
    "    if not all_points or not all_points[0]:\n",
    "        logger.warning(\"No points found in the collection. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"Embeddings array is empty. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "    logger.info(f\"KNN index built successfully with {len(point_id_mapping)} points.\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata.get('tfidf_similarity', 0.0),\n",
    "            section.metadata.get('semantic_similarity', 0.0),\n",
    "            section.metadata.get('centrality', 0.0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model is None:\n",
    "        logger.warning(\"KNN model is not built. No search can be performed.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "    logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    build_knn_index()  # This will now handle empty collections gracefully\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Using cached xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: xgboost in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (1.6.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: sentence-transformers in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (2.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx>=0.20.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.26.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (69.5.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.3)\n",
      "Requirement already satisfied: h2<5,>=3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Rolling back uninstall of protobuf\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\json_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\json_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message_factory.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message_factory.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\any_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\any_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\api_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\api_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\compiler\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~ompiler\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pool.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pool.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\duration_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\duration_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\empty_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\empty_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\field_mask_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\field_mask_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\_parameterized.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\_parameterized.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\api_implementation.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\api_implementation.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\containers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\decoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\encoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\enum_type_wrapper.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\enum_type_wrapper.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\extension_dict.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\extension_dict.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\field_mask.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\field_mask.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\message_listener.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\message_listener.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\numpy\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\internal\\~umpy\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\python_message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\type_checkers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\well_known_types.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\wire_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\wire_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\json_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\json_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message_factory.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message_factory.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\proto_builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\proto_builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\pyext\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~yext\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service_reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service_reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\source_context_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\source_context_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\struct_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\struct_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\symbol_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_encoding.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_encoding.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\timestamp_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\timestamp_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\type_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\type_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\unknown_fields.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\unknown_fields.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\util\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~til\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\wrappers_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\wrappers_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-u_83ebsj\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3.dist-info\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~rotobuf-4.25.3.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Users\\\\nasan\\\\anaconda3\\\\envs\\\\myenv\\\\Lib\\\\site-packages\\\\google\\\\_upb\\\\_message.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' already exists.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' exists.\n",
      "INFO:__main__:Training XGBRanker is not implemented. Using default model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [35356]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 768  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; adjust based on SentenceTransformer model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000, offset=scroll_response.next_page_offset)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 768  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; will be updated based on model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(\n",
    "                    collection_name=COLLECTION_NAME_README,\n",
    "                    limit=10000,\n",
    "                    offset=scroll_response.next_page_offset\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' already exists.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' exists.\n",
      "INFO:__main__:Training XGBRanker is not implemented. Using default model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [45488]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 768  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; will be updated based on model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(\n",
    "                    collection_name=COLLECTION_NAME_README,\n",
    "                    limit=10000,\n",
    "                    offset=scroll_response.next_page_offset\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   python setup.py egg_info did not run successfully.\n",
      "   exit code: 1\n",
      "  > [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      " Encountered error while generating package metadata.\n",
      "> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn qdrant-client markdown beautifulsoup4 sklearn xgboost networkx nest_asyncio sentence-transformers numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
